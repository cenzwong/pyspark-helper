{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p> <p>GitHub</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"debug/","title":"Debug","text":""},{"location":"debug/#pysparky.debug.get_distinct_value_from_df_columns","title":"<code>get_distinct_value_from_df_columns(df, columns_names, display=True)</code>","text":"<p>Get distinct values from specified DataFrame columns and optionally display their counts.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>columns_names</code> <code>list[str]</code> <p>List of column names to process.</p> required <code>display</code> <code>bool</code> <p>Whether to display the counts of distinct values. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, list]</code> <p>dict[str, list]: A dictionary where keys are column names and values are lists of distinct values.</p> Source code in <code>pysparky/debug.py</code> <pre><code>def get_distinct_value_from_df_columns(\n    df: DataFrame, columns_names: list[str], display: bool = True\n) -&gt; dict[str, list]:\n    \"\"\"\n    Get distinct values from specified DataFrame columns and optionally display their counts.\n\n    Args:\n        df (DataFrame): The input Spark DataFrame.\n        columns_names (list[str]): List of column names to process.\n        display (bool): Whether to display the counts of distinct values. Default is True.\n\n    Returns:\n        dict[str, list]: A dictionary where keys are column names and values are lists of distinct values.\n    \"\"\"\n    myDict = {}\n    for col in columns_names:\n        data = df.select(col).distinct()\n        myDict[col] = [row[col] for row in data.collect()]\n\n        if display:\n            if df.groupBy(col).count().count() &lt; 20:\n                df.groupBy(col).count().show()\n    return myDict\n</code></pre>"},{"location":"decorator/","title":"Decorator","text":""},{"location":"decorator/#pysparky.decorator.column_name_or_column_names_enabler","title":"<code>column_name_or_column_names_enabler(*param_names)</code>","text":"<p>A decorator to enable PySpark functions to accept either column names (as strings) or Column objects.</p> <p>Parameters: param_names (str): Names of the parameters that should be converted from strings to Column objects.</p> <p>Returns: function: The decorated function with specified parameters converted to Column objects if they are strings.</p> <p>Example @pyspark_column_or_name_enabler(\"column_or_name\") def your_function(column_or_name):     return column_or_name.startswith(bins)</p> Source code in <code>pysparky/decorator.py</code> <pre><code>def column_name_or_column_names_enabler(*param_names):\n    \"\"\"\n    A decorator to enable PySpark functions to accept either column names (as strings) or Column objects.\n\n    Parameters:\n    param_names (str): Names of the parameters that should be converted from strings to Column objects.\n\n    Returns:\n    function: The decorated function with specified parameters converted to Column objects if they are strings.\n\n    Example\n    @pyspark_column_or_name_enabler(\"column_or_name\")\n    def your_function(column_or_name):\n        return column_or_name.startswith(bins)\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert args to a list to modify them\n            # args: This is the list of argument of the function.\n            # Get the parameter indices from the function signature\n            # func.__code__.co_varnames : Return the function parameter names as a tuple.\n            # param_names : the list of parameter from the decorator\n\n            # Merging the args into Kwargs\n            args_name_used = func.__code__.co_varnames[: len(args)]\n            kw_from_args = dict(zip(args_name_used, args))\n            kwargs = kw_from_args | kwargs\n\n            # print(kwargs)\n            # transform all the input param\n            for param_name in param_names:\n                # if it is string, wrap it as string, else do nth\n                kwargs[param_name] = (\n                    [kwargs[param_name]]\n                    if isinstance(kwargs[param_name], str)\n                    else kwargs[param_name]\n                )\n\n            return func(**kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"decorator/#pysparky.decorator.extension_enabler","title":"<code>extension_enabler(cls)</code>","text":"<p>This enable you to chain the class</p> Source code in <code>pysparky/decorator.py</code> <pre><code>def extension_enabler(cls):\n    \"\"\"\n    This enable you to chain the class\n    \"\"\"\n\n    def decorator(func):\n        # assign the function into the object\n        setattr(cls, f\"{func.__name__}\", func)\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            func_return = func(*args, **kwargs)\n            return func_return\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"decorator/#pysparky.decorator.pyspark_column_or_name_enabler","title":"<code>pyspark_column_or_name_enabler(*param_names)</code>","text":"<p>A decorator to enable PySpark functions to accept either column names (as strings) or Column objects.</p> <p>Parameters: param_names (str): Names of the parameters that should be converted from strings to Column objects.</p> <p>Returns: function: The decorated function with specified parameters converted to Column objects if they are strings.</p> <p>Example @pyspark_column_or_name_enabler(\"column_or_name\") def your_function(column_or_name):     return column_or_name.startswith(bins)</p> Source code in <code>pysparky/decorator.py</code> <pre><code>def pyspark_column_or_name_enabler(*param_names):\n    \"\"\"\n    A decorator to enable PySpark functions to accept either column names (as strings) or Column objects.\n\n    Parameters:\n    param_names (str): Names of the parameters that should be converted from strings to Column objects.\n\n    Returns:\n    function: The decorated function with specified parameters converted to Column objects if they are strings.\n\n    Example\n    @pyspark_column_or_name_enabler(\"column_or_name\")\n    def your_function(column_or_name):\n        return column_or_name.startswith(bins)\n    \"\"\"\n\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Convert args to a list to modify them\n            # args: This is the list of argument of the function.\n            # Get the parameter indices from the function signature\n            # func.__code__.co_varnames : Return the function parameter names as a tuple.\n            # param_names : the list of parameter from the decorator\n\n            # Merging the args into Kwargs\n            args_name_used = func.__code__.co_varnames[: len(args)]\n            kw_from_args = dict(zip(args_name_used, args))\n            kwargs = kw_from_args | kwargs\n\n            # print(kwargs)\n            # transform all the input param\n            for param_name in param_names:\n                # if it is string, wrap it as string, else do nth\n                kwargs[param_name] = (\n                    F.col(kwargs[param_name])\n                    if isinstance(kwargs[param_name], str)\n                    else kwargs[param_name]\n                )\n\n            return func(**kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"functions/","title":"Functions","text":""},{"location":"functions/#pysparky.functions.general.chain","title":"<code>chain(self, func, *args, **kwargs)</code>","text":"<p>Applies a given function to the current Column and returns the result.</p> <p>This method allows for chaining operations on a Column object by applying a custom function with additional arguments. It's particularly useful for creating complex transformations or applying user-defined functions to a Column.</p> <p>Parameters:</p> Name Type Description Default <code>self</code> <code>Column</code> <p>The current Column object.</p> required <code>func</code> <code>callable</code> <p>The function to apply to the Column.</p> required <code>*args</code> <p>Variable length argument list to pass to the function.</p> <code>()</code> <code>**kwargs</code> <p>Arbitrary keyword arguments to pass to the function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A new Column object resulting from applying the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; df = spark.createDataFrame([(\"hello\",)], [\"text\"])\n&gt;&gt;&gt; def custom_upper(col):\n...     return F.upper(col)\n&gt;&gt;&gt; result = df.withColumn(\"upper_text\", df.text.chain(custom_upper))\n&gt;&gt;&gt; result.show()\n+-----+----------+\n| text|upper_text|\n+-----+----------+\n|hello|     HELLO|\n+-----+----------+\n</code></pre> <pre><code>&gt;&gt;&gt; def add_prefix(col, prefix):\n...     return F.concat(F.lit(prefix), col)\n&gt;&gt;&gt; result = df.withColumn(\"prefixed_text\", df.text.chain(add_prefix, prefix=\"Pre: \"))\n&gt;&gt;&gt; result.show()\n+-----+-------------+\n| text|prefixed_text|\n+-----+-------------+\n|hello|   Pre: hello|\n+-----+-------------+\n</code></pre> Note <p>The function passed to <code>chain</code> should expect a Column as its first argument, followed by any additional arguments specified in the <code>chain</code> call.</p> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef chain(self, func, *args, **kwargs) -&gt; Column:\n    \"\"\"\n    Applies a given function to the current Column and returns the result.\n\n    This method allows for chaining operations on a Column object by applying\n    a custom function with additional arguments. It's particularly useful for\n    creating complex transformations or applying user-defined functions to a Column.\n\n    Args:\n        self (Column): The current Column object.\n        func (callable): The function to apply to the Column.\n        *args: Variable length argument list to pass to the function.\n        **kwargs: Arbitrary keyword arguments to pass to the function.\n\n    Returns:\n        Column: A new Column object resulting from applying the function.\n\n    Examples:\n        &gt;&gt;&gt; df = spark.createDataFrame([(\"hello\",)], [\"text\"])\n        &gt;&gt;&gt; def custom_upper(col):\n        ...     return F.upper(col)\n        &gt;&gt;&gt; result = df.withColumn(\"upper_text\", df.text.chain(custom_upper))\n        &gt;&gt;&gt; result.show()\n        +-----+----------+\n        | text|upper_text|\n        +-----+----------+\n        |hello|     HELLO|\n        +-----+----------+\n\n        &gt;&gt;&gt; def add_prefix(col, prefix):\n        ...     return F.concat(F.lit(prefix), col)\n        &gt;&gt;&gt; result = df.withColumn(\"prefixed_text\", df.text.chain(add_prefix, prefix=\"Pre: \"))\n        &gt;&gt;&gt; result.show()\n        +-----+-------------+\n        | text|prefixed_text|\n        +-----+-------------+\n        |hello|   Pre: hello|\n        +-----+-------------+\n\n    Note:\n        The function passed to `chain` should expect a Column as its first argument,\n        followed by any additional arguments specified in the `chain` call.\n    \"\"\"\n    return func(self, *args, **kwargs)\n</code></pre>"},{"location":"functions/#pysparky.functions.general.get_value_from_map","title":"<code>get_value_from_map(column_or_name, dict_)</code>","text":"<p>Retrieves a value from a map (dictionary) using a key derived from a specified column in a DataFrame.</p> <p>This function creates a map from the provided dictionary and then looks up the value in the map corresponding to the key that matches the value in the specified column.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>str</code> <p>The name of the column in the DataFrame whose value will be used as the key to look up in the map.</p> required <code>dict_</code> <code>dict</code> <p>A dictionary where keys and values are the elements to be used in the map.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A PySpark Column object representing the value retrieved from the map.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; map = {1: 'a', 2: 'b'}\n&gt;&gt;&gt; column_name = 'key_column'\n&gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,)], ['key_column'])\n&gt;&gt;&gt; df.withColumn('value', get_value_from_map(map, column_name)).show()\n+----------+-----+\n|key_column|value|\n+----------+-----+\n|         1|    a|\n|         2|    b|\n+----------+-----+\n</code></pre> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\n@decorator.pyspark_column_or_name_enabler(\"column_or_name\")\ndef get_value_from_map(column_or_name: str | Column, dict_: dict) -&gt; Column:\n    \"\"\"\n    Retrieves a value from a map (dictionary) using a key derived from a specified column in a DataFrame.\n\n    This function creates a map from the provided dictionary and then looks up the value in the map\n    corresponding to the key that matches the value in the specified column.\n\n    Args:\n        column_or_name (str): The name of the column in the DataFrame whose value will be used as the key to look up in the map.\n        dict_ (dict): A dictionary where keys and values are the elements to be used in the map.\n\n    Returns:\n        Column: A PySpark Column object representing the value retrieved from the map.\n\n    Examples:\n        &gt;&gt;&gt; map = {1: 'a', 2: 'b'}\n        &gt;&gt;&gt; column_name = 'key_column'\n        &gt;&gt;&gt; df = spark.createDataFrame([(1,), (2,)], ['key_column'])\n        &gt;&gt;&gt; df.withColumn('value', get_value_from_map(map, column_name)).show()\n        +----------+-----+\n        |key_column|value|\n        +----------+-----+\n        |         1|    a|\n        |         2|    b|\n        +----------+-----+\n    \"\"\"\n    return utils.create_map_from_dict(dict_)[column_or_name]\n</code></pre>"},{"location":"functions/#pysparky.functions.general.lower_","title":"<code>lower_(col)</code>","text":"<p>This serve as an easy Examples on how this package work</p> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\ndef lower_(col: Column) -&gt; Column:\n    \"\"\"\n    This serve as an easy Examples on how this package work\n    \"\"\"\n    return F.lower(col)\n</code></pre>"},{"location":"functions/#pysparky.functions.general.replace_strings_to_none","title":"<code>replace_strings_to_none(column_or_name, list_of_null_string, customize_output=None)</code>","text":"<p>Replaces empty string values in a column with None.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>ColumnOrName</code> <p>The column to check for empty string values.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A Spark DataFrame column with the values replaced.</p> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\n@decorator.pyspark_column_or_name_enabler(\"column_or_name\")\ndef replace_strings_to_none(\n    column_or_name: str | Column,\n    list_of_null_string: list[str],\n    customize_output: Any = None,\n) -&gt; pyspark.sql.Column:\n    \"\"\"\n    Replaces empty string values in a column with None.\n\n    Args:\n        column_or_name (ColumnOrName): The column to check for empty string values.\n\n    Returns:\n        Column: A Spark DataFrame column with the values replaced.\n    \"\"\"\n\n    return F.when(column_or_name.isin(list_of_null_string), customize_output).otherwise(\n        column_or_name\n    )\n</code></pre>"},{"location":"functions/#pysparky.functions.general.single_space_and_trim","title":"<code>single_space_and_trim(column_or_name)</code>","text":"<p>Replaces multiple white spaces with a single space and trims the column.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>Column</code> <p>The column to be adjusted.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A trimmed column with single spaces.</p> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\n@decorator.pyspark_column_or_name_enabler(\"column_or_name\")\ndef single_space_and_trim(column_or_name: str | Column) -&gt; Column:\n    \"\"\"\n    Replaces multiple white spaces with a single space and trims the column.\n\n    Args:\n        column_or_name (Column): The column to be adjusted.\n\n    Returns:\n        Column: A trimmed column with single spaces.\n    \"\"\"\n\n    return F.trim(F.regexp_replace(column_or_name, r\"\\s+\", \" \"))\n</code></pre>"},{"location":"functions/#pysparky.functions.general.startswiths","title":"<code>startswiths(column_or_name, list_of_strings)</code>","text":"<p>Creates a PySpark Column expression to check if the given column starts with any string in the list.</p> <p>Parameters:</p> Name Type Description Default <code>column_or_name</code> <code>ColumnOrName</code> <p>The column to check.</p> required <code>list_of_strings</code> <code>List[str]</code> <p>A list of strings to check if the column starts with.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A PySpark Column expression that evaluates to True if the column starts with any string in the list, otherwise False.</p> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\n@decorator.pyspark_column_or_name_enabler(\"column_or_name\")\ndef startswiths(\n    column_or_name: str | Column, list_of_strings: list[str]\n) -&gt; pyspark.sql.Column:\n    \"\"\"\n    Creates a PySpark Column expression to check if the given column starts with any string in the list.\n\n    Args:\n        column_or_name (ColumnOrName): The column to check.\n        list_of_strings (List[str]): A list of strings to check if the column starts with.\n\n    Returns:\n        Column: A PySpark Column expression that evaluates to True if the column starts with any string in the list, otherwise False.\n    \"\"\"\n\n    return functools.reduce(\n        operator.or_,\n        map(column_or_name.startswith, list_of_strings),\n        F.lit(False),\n    ).alias(f\"startswiths_len{len(list_of_strings)}\")\n</code></pre>"},{"location":"functions/#pysparky.functions.general.when_mapping","title":"<code>when_mapping(column_or_name, dict_)</code>","text":"<p>Applies a series of conditional mappings to a PySpark Column based on a dictionary of conditions and values.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>Column</code> <p>The PySpark Column to which the conditional mappings will be applied.</p> required <code>dict_</code> <code>Dict</code> <p>A dictionary where keys are the conditions and values are the corresponding results.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A new PySpark Column with the conditional mappings applied.</p> Source code in <code>pysparky/functions/general.py</code> <pre><code>@decorator.extension_enabler(Column)\n@decorator.pyspark_column_or_name_enabler(\"column_or_name\")\ndef when_mapping(column_or_name: Column, dict_: dict) -&gt; Column:\n    \"\"\"\n    Applies a series of conditional mappings to a PySpark Column based on a dictionary of conditions and values.\n\n    Args:\n        column (Column): The PySpark Column to which the conditional mappings will be applied.\n        dict_ (Dict): A dictionary where keys are the conditions and values are the corresponding results.\n\n    Returns:\n        Column: A new PySpark Column with the conditional mappings applied.\n    \"\"\"\n    result_column = F  # initiate as an functions\n    for condition, value in dict_.items():\n        result_column = result_column.when(column_or_name == condition, value)\n    return result_column\n</code></pre>"},{"location":"functions/#pysparky.functions.math_.cumsum","title":"<code>cumsum(columnOrName, partition_by=None, order_by_column=None, is_normalized=False, is_descending=False, alias='cumsum')</code>","text":"<p>Calculate the cumulative sum of a column, optionally partitioned by other columns.</p> <p>Parameters:</p> Name Type Description Default <code>columnOrName</code> <code>Column</code> <p>The column for which to calculate the cumulative sum.</p> required <code>partition_by</code> <code>list[Column]</code> <p>A list of columns to partition by. Defaults to an empty list.</p> <code>None</code> <code>order_by_column</code> <code>Column</code> <p>The Column for order by, null for using the same column.</p> <code>None</code> <code>is_normalized</code> <code>bool</code> <p>Whether to normalize the cumulative sum. Defaults to False.</p> <code>False</code> <code>is_descending</code> <code>bool</code> <p>Whether to order the cumulative sum in descending order. Defaults to False.</p> <code>False</code> <code>alias</code> <code>str</code> <p>Alias for the resulting column. Defaults to \"cumsum\".</p> <code>'cumsum'</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A column representing the cumulative sum.</p> Example <p>df = spark.createDataFrame([(1, \"A\", 10), (2, \"A\", 20), (3, \"B\", 30)], [\"id\", \"category\", \"value\"]) result_df = df.select(\"id\", \"category\", \"value\", cumsum(F.col(\"value\"), partition_by=[F.col(\"category\")], is_descending=True)) result_df.display()</p> Source code in <code>pysparky/functions/math_.py</code> <pre><code>@decorator.pyspark_column_or_name_enabler(\"columnOrName\")\ndef cumsum(\n    columnOrName: Column,\n    partition_by: list[Column] = None,\n    order_by_column: Column = None,\n    is_normalized: bool = False,\n    is_descending: bool = False,\n    alias: str = \"cumsum\",\n) -&gt; Column:\n    \"\"\"\n    Calculate the cumulative sum of a column, optionally partitioned by other columns.\n\n    Args:\n        columnOrName (Column): The column for which to calculate the cumulative sum.\n        partition_by (list[Column], optional): A list of columns to partition by. Defaults to an empty list.\n        order_by_column: The Column for order by, null for using the same column.\n        is_normalized (bool, optional): Whether to normalize the cumulative sum. Defaults to False.\n        is_descending (bool, optional): Whether to order the cumulative sum in descending order. Defaults to False.\n        alias (str, optional): Alias for the resulting column. Defaults to \"cumsum\".\n\n    Returns:\n        Column: A column representing the cumulative sum.\n\n    Example:\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, \"A\", 10), (2, \"A\", 20), (3, \"B\", 30)], [\"id\", \"category\", \"value\"])\n        &gt;&gt;&gt; result_df = df.select(\"id\", \"category\", \"value\", cumsum(F.col(\"value\"), partition_by=[F.col(\"category\")], is_descending=True))\n        &gt;&gt;&gt; result_df.display()\n    \"\"\"\n    if partition_by is None:\n        partition_by = []\n    if order_by_column is None:\n        order_by_column = columnOrName\n\n    if is_normalized:\n        total_sum = F.sum(columnOrName).over(Window.partitionBy(partition_by))\n    else:\n        total_sum = F.lit(1)\n\n    if is_descending:\n        order_by_column_ordered = order_by_column.desc()\n    else:\n        order_by_column_ordered = order_by_column.asc()\n\n    cumsum_ = F.sum(columnOrName).over(\n        Window.partitionBy(partition_by).orderBy(order_by_column_ordered)\n    )\n\n    return (cumsum_ / total_sum).alias(alias)\n</code></pre>"},{"location":"functions/#pysparky.functions.math_.haversine_distance","title":"<code>haversine_distance(lat1, long1, lat2, long2)</code>","text":"<p>Calculates the Haversine distance between two sets of latitude and longitude coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>lat1</code> <code>Column</code> <p>The column containing the latitude of the first coordinate.</p> required <code>long1</code> <code>Column</code> <p>The column containing the longitude of the first coordinate.</p> required <code>lat2</code> <code>Column</code> <p>The column containing the latitude of the second coordinate.</p> required <code>long2</code> <code>Column</code> <p>The column containing the longitude of the second coordinate.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>The column containing the calculated Haversine distance.</p> <p>Examples:</p> <pre><code>haversine_distance(F.lit(52.1552), F.lit(5.3876), F.lit(59.9111), F.lit(10.7503))\n923.8038067341608\n</code></pre> Source code in <code>pysparky/functions/math_.py</code> <pre><code>@decorator.extension_enabler(Column)\n@decorator.pyspark_column_or_name_enabler(\"lat1\", \"long1\", \"lat2\", \"long2\")\ndef haversine_distance(\n    lat1: Column,\n    long1: Column,\n    lat2: Column,\n    long2: Column,\n) -&gt; Column:\n    \"\"\"\n    Calculates the Haversine distance between two sets of latitude and longitude coordinates.\n\n    Args:\n        lat1 (Column): The column containing the latitude of the first coordinate.\n        long1 (Column): The column containing the longitude of the first coordinate.\n        lat2 (Column): The column containing the latitude of the second coordinate.\n        long2 (Column): The column containing the longitude of the second coordinate.\n\n    Returns:\n        Column: The column containing the calculated Haversine distance.\n\n    Examples:\n        ```python\n        haversine_distance(F.lit(52.1552), F.lit(5.3876), F.lit(59.9111), F.lit(10.7503))\n        923.8038067341608\n        ```\n    \"\"\"\n    # Convert latitude and longitude from degrees to radians\n    lat1_randians = F.radians(lat1)\n    long1_randians = F.radians(long1)\n    lat2_randians = F.radians(lat2)\n    long2_randians = F.radians(long2)\n\n    # Haversine formula\n    dlat = lat2_randians - lat1_randians\n    dlong = long2_randians - long1_randians\n    a = (\n        F.sin(dlat / 2) ** 2\n        + F.cos(lat1_randians) * F.cos(lat2_randians) * F.sin(dlong / 2) ** 2\n    )\n    c = 2 * F.atan2(F.sqrt(a), F.sqrt(1 - a))\n\n    # Radius of the Earth (in kilometers)\n    R = 6371.0\n\n    # Calculate the distance\n    distance = F.round(R * c, 4)\n\n    return distance.alias(\"haversine_distance\")\n</code></pre>"},{"location":"functions/#pysparky.functions.conditions.condition_and","title":"<code>condition_and(*conditions)</code>","text":"<p>Combines multiple conditions using logical AND.</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>Union[Column, str]</code> <p>Multiple PySpark Column objects or SQL expression strings representing conditions.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A single PySpark Column object representing the combined condition.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; condition_and(F.col('col1') &gt; 1, F.col('col2') &lt; 5)\nColumn&lt;'((col1 &gt; 1) AND (col2 &lt; 5))'&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; condition_and(F.col('col1') &gt; 1, \"col2 &lt; 5\")\nColumn&lt;'((col1 &gt; 1) AND (col2 &lt; 5))'&gt;\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>def condition_and(*conditions: Union[Column, str]) -&gt; Column:\n    \"\"\"\n    Combines multiple conditions using logical AND.\n\n    Args:\n        *conditions (Union[Column, str]): Multiple PySpark Column objects or SQL expression strings representing conditions.\n\n    Returns:\n        Column: A single PySpark Column object representing the combined condition.\n\n    Examples:\n        &gt;&gt;&gt; condition_and(F.col('col1') &gt; 1, F.col('col2') &lt; 5)\n        Column&lt;'((col1 &gt; 1) AND (col2 &lt; 5))'&gt;\n\n        &gt;&gt;&gt; condition_and(F.col('col1') &gt; 1, \"col2 &lt; 5\")\n        Column&lt;'((col1 &gt; 1) AND (col2 &lt; 5))'&gt;\n    \"\"\"\n    parsed_conditions = [\n        F.expr(cond) if isinstance(cond, str) else cond for cond in conditions\n    ]\n    return reduce(and_, parsed_conditions, F.lit(True))\n</code></pre>"},{"location":"functions/#pysparky.functions.conditions.condition_or","title":"<code>condition_or(*conditions)</code>","text":"<p>Combines multiple conditions using logical OR.</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>Union[Column, str]</code> <p>Multiple PySpark Column objects or SQL expression strings representing conditions.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A single PySpark Column object representing the combined condition.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; condition_or(F.col('col1') &gt; 1, F.col('col2') &lt; 5)\nColumn&lt;'((col1 &gt; 1) OR (col2 &lt; 5))'&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; condition_or(F.col('col1') &gt; 1, \"col2 &lt; 5\")\nColumn&lt;'((col1 &gt; 1) OR (col2 &lt; 5))'&gt;\n</code></pre> Source code in <code>pysparky/functions/conditions.py</code> <pre><code>def condition_or(*conditions: Union[Column, str]) -&gt; Column:\n    \"\"\"\n    Combines multiple conditions using logical OR.\n\n    Args:\n        *conditions (Union[Column, str]): Multiple PySpark Column objects or SQL expression strings representing conditions.\n\n    Returns:\n        Column: A single PySpark Column object representing the combined condition.\n\n    Examples:\n        &gt;&gt;&gt; condition_or(F.col('col1') &gt; 1, F.col('col2') &lt; 5)\n        Column&lt;'((col1 &gt; 1) OR (col2 &lt; 5))'&gt;\n\n        &gt;&gt;&gt; condition_or(F.col('col1') &gt; 1, \"col2 &lt; 5\")\n        Column&lt;'((col1 &gt; 1) OR (col2 &lt; 5))'&gt;\n    \"\"\"\n    parsed_conditions = [\n        F.expr(cond) if isinstance(cond, str) else cond for cond in conditions\n    ]\n    return reduce(or_, parsed_conditions, F.lit(False))\n</code></pre>"},{"location":"quality/","title":"Quality","text":""},{"location":"quality/#pysparky.quality.expect_any_to_one","title":"<code>expect_any_to_one(col1, col2)</code>","text":"<p>A decorator function that ensures an N:1 relationship between col1 and col2, meaning each value in col1 corresponds to only one distinct value in col2.</p> <p>Parameters:</p> Name Type Description Default <code>col1</code> <code>str | Sequence[str]</code> <p>Name of the column or a tuple of column names.</p> required <code>col2</code> <code>str | Sequence[str]</code> <p>Name of the column or a tuple of column names.</p> required Source code in <code>pysparky/quality.py</code> <pre><code>def expect_any_to_one(col1: str | Sequence[str], col2: str | Sequence[str]):\n    \"\"\"\n    A decorator function that ensures an N:1 relationship between col1 and col2,\n    meaning each value in col1 corresponds to only one distinct value in col2.\n\n    Args:\n        col1 (str | Sequence[str]): Name of the column or a tuple of column names.\n        col2 (str | Sequence[str]): Name of the column or a tuple of column names.\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            spark_table_sdf = func(*args, **kwargs)\n            num_col2_values_with_many_col1_values = (\n                spark_table_sdf.groupBy(*col1)\n                .agg(F.count_distinct(*col2).alias(\"distinct_count\"))\n                .where(F.col(\"distinct_count\") &gt; 1)\n                .count()\n            )\n            assert (\n                num_col2_values_with_many_col1_values == 0\n            ), f\"Multiple {col2}s per {col1}\"\n            print(f\"\u2705: {col1}:{col2} is N:1\")\n            return spark_table_sdf\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"quality/#pysparky.quality.expect_criteria","title":"<code>expect_criteria(criteria)</code>","text":"<p>A decorator function that ensures a specific criterion on a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Column</code> <p>The filter criterion to be applied to the DataFrame.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorated function that checks the criterion.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the filtered count and unfiltered count of the DataFrame are not equal.</p> Source code in <code>pysparky/quality.py</code> <pre><code>def expect_criteria(criteria):\n    \"\"\"\n    A decorator function that ensures a specific criterion on a Spark DataFrame.\n\n    Parameters:\n        criteria (pyspark.sql.column.Column): The filter criterion to be applied to the DataFrame.\n\n    Returns:\n        function: A decorated function that checks the criterion.\n\n    Raises:\n        AssertionError: If the filtered count and unfiltered count of the DataFrame are not equal.\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            spark_table_sdf = func(*args, **kwargs)\n            filtered_count = spark_table_sdf.filter(criteria).count()\n            unfiltered_count = spark_table_sdf.count()\n            assert (\n                filtered_count == unfiltered_count\n            ), f\"Filtered count is not equal to unfiltered count {criteria}\"\n            print(f\"\u2705: Criteria '{criteria}' passed\")\n            return spark_table_sdf\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"quality/#pysparky.quality.expect_one_to_one","title":"<code>expect_one_to_one(col1, col2)</code>","text":"<p>A decorator function that ensures a 1:1 relationship between col1 and col2, meaning each value in col1 corresponds to only one distinct value in col2 and vice-versa.</p> <p>Parameters:</p> Name Type Description Default <code>col1</code> <code>str | Sequence[str]</code> <p>Name of the column or a tuple of column names.</p> required <code>col2</code> <code>str | Sequence[str]</code> <p>Name of the column or a tuple of column names.</p> required Source code in <code>pysparky/quality.py</code> <pre><code>def expect_one_to_one(col1: str | Sequence[str], col2: str | Sequence[str]):\n    \"\"\"\n    A decorator function that ensures a 1:1 relationship between col1 and col2,\n    meaning each value in col1 corresponds to only one distinct value in col2 and vice-versa.\n\n    Args:\n        col1 (str | Sequence[str]): Name of the column or a tuple of column names.\n        col2 (str | Sequence[str]): Name of the column or a tuple of column names.\n    \"\"\"\n\n    any_to_one = expect_any_to_one(col1, col2)\n    one_to_any = expect_any_to_one(col2, col1)\n\n    def decorator(func):\n        return one_to_any(any_to_one(func))\n\n    return decorator\n</code></pre>"},{"location":"quality/#pysparky.quality.expect_type","title":"<code>expect_type(col_name, col_type)</code>","text":"<p>A decorator function that verifies the data type of a specified column in a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The column's name.</p> required <code>col_type</code> <code>DataType</code> <p>The expected data type for the column.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorated function that checks the column's data type.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the column's data type does not match the expected type.</p> Source code in <code>pysparky/quality.py</code> <pre><code>def expect_type(col_name, col_type):\n    \"\"\"\n    A decorator function that verifies the data type of a specified column in a Spark DataFrame.\n\n    Parameters:\n        col_name (str): The column's name.\n        col_type (pyspark.sql.types.DataType): The expected data type for the column.\n\n    Returns:\n        function: A decorated function that checks the column's data type.\n\n    Raises:\n        AssertionError: If the column's data type does not match the expected type.\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            spark_table_sdf = func(*args, **kwargs)\n            source_type = spark_table_sdf.schema[col_name].dataType\n            target_type = col_type\n            assert (\n                source_type == target_type\n            ), f\"Data type of column '{col_name}:{source_type}' is not equal to {target_type}\"  # noqa: E501\n            print(f\"\u2705: Column '{col_name}' has the expected data type {col_type}\")\n            return spark_table_sdf\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"quality/#pysparky.quality.expect_unique","title":"<code>expect_unique(col_name)</code>","text":"<p>A decorator function that ensures the uniqueness of a column in a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>The column's name.</p> required <p>Returns:</p> Name Type Description <code>function</code> <p>A decorated function that checks the column's uniqueness.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the column's count and distinct count are not equal.</p> Source code in <code>pysparky/quality.py</code> <pre><code>def expect_unique(col_name):\n    \"\"\"\n    A decorator function that ensures the uniqueness of a column in a Spark DataFrame.\n\n    Parameters:\n        col_name (str): The column's name.\n\n    Returns:\n        function: A decorated function that checks the column's uniqueness.\n\n    Raises:\n        AssertionError: If the column's count and distinct count are not equal.\n    \"\"\"\n\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            spark_table_sdf = func(*args, **kwargs)\n            spark_table_col_sdf = spark_table_sdf.select(col_name)\n            normal_count = spark_table_col_sdf.count()\n            distinct_count = spark_table_col_sdf.distinct().count()\n            assert (\n                normal_count == distinct_count\n            ), f\"Count and distinct count of column '{col_name}' are not equal\"\n            print(f\"\u2705: Column '{col_name}' is distinct\")\n            return spark_table_sdf\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"schema_ext/","title":"Schema ext","text":""},{"location":"schema_ext/#pysparky.schema_ext.filter_columns_by_datatype","title":"<code>filter_columns_by_datatype(struct_type, data_type)</code>","text":"<p>Filters and returns a StructType of StructField names from a given StructType schema that match the specified data type.</p> <p>Parameters:</p> Name Type Description Default <code>struct_type</code> <code>StructType</code> <p>The schema of the DataFrame.</p> required <code>data_type</code> <code>DataType</code> <p>The data type to filter by.</p> required <p>Returns:</p> Type Description <code>StructType</code> <p>T.StructType: A StructType of StructField names that match the specified data type.</p> Source code in <code>pysparky/schema_ext.py</code> <pre><code>@decorator.extension_enabler(T.StructType)\ndef filter_columns_by_datatype(\n    struct_type: T.StructType, data_type: T.DataType\n) -&gt; T.StructType:\n    \"\"\"\n    Filters and returns a StructType of StructField names from a given StructType schema\n    that match the specified data type.\n\n    Args:\n        struct_type (T.StructType): The schema of the DataFrame.\n        data_type (T.DataType): The data type to filter by.\n\n    Returns:\n        T.StructType: A StructType of StructField names that match the specified data type.\n    \"\"\"\n    return T.StructType([field for field in struct_type if field.dataType == data_type])\n</code></pre>"},{"location":"spark_ext/","title":"Spark ext","text":""},{"location":"spark_ext/#pysparky.spark_ext.column_function","title":"<code>column_function(spark, column_obj)</code>","text":"<p>Evaluates a Column expression in the context of a single-row DataFrame.</p> <p>This function creates a DataFrame with a single row and applies the given Column expression to it. This is particularly useful for testing Column expressions, evaluating complex transformations, or creating sample data based on Column operations.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <code>SparkSession</code> <p>The SparkSession object.</p> required <code>column_obj</code> <code>Column</code> <p>The Column object or expression to evaluate.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A single-row DataFrame containing the result of the Column expression.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F\n&gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.column_function--simple-column-expression","title":"Simple column expression","text":"<pre><code>&gt;&gt;&gt; result = spark.column_function(F.lit(\"Hello, World!\"))\n&gt;&gt;&gt; result.show()\n+-------------+\n|         col0|\n+-------------+\n|Hello, World!|\n+-------------+\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.column_function--complex-column-expression","title":"Complex column expression","text":"<pre><code>&gt;&gt;&gt; import datetime\n&gt;&gt;&gt; complex_col = F.when(F.current_date() &gt; F.lit(datetime.date(2023, 1, 1)), \"Future\")\n...                .otherwise(\"Past\")\n&gt;&gt;&gt; result = spark.column_function(complex_col)\n&gt;&gt;&gt; result.show()\n+------+\n|  col0|\n+------+\n|Future|\n+------+\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.column_function--using-with-user-defined-functions-udfs","title":"Using with user-defined functions (UDFs)","text":"<pre><code>&gt;&gt;&gt; from pyspark.sql.types import IntegerType\n&gt;&gt;&gt; square_udf = F.udf(lambda x: x * x, IntegerType())\n&gt;&gt;&gt; result = spark.column_function(square_udf(F.lit(5)))\n&gt;&gt;&gt; result.show()\n+----+\n|col0|\n+----+\n|  25|\n+----+\n</code></pre> Notes <ul> <li>This function is particularly useful for debugging or testing Column expressions   without the need to create a full DataFrame.</li> <li>The resulting DataFrame will always have a single column named 'col0' unless   the input Column object has a specific alias.</li> <li>Be cautious when using this with resource-intensive operations, as it still   creates a distributed DataFrame operation.</li> </ul> Source code in <code>pysparky/spark_ext.py</code> <pre><code>@decorator.extension_enabler(SparkSession)\ndef column_function(spark, column_obj: Column) -&gt; DataFrame:\n    \"\"\"\n    Evaluates a Column expression in the context of a single-row DataFrame.\n\n    This function creates a DataFrame with a single row and applies the given Column\n    expression to it. This is particularly useful for testing Column expressions,\n    evaluating complex transformations, or creating sample data based on Column operations.\n\n    Args:\n        spark (pyspark.sql.SparkSession): The SparkSession object.\n        column_obj (pyspark.sql.Column): The Column object or expression to evaluate.\n\n    Returns:\n        pyspark.sql.DataFrame: A single-row DataFrame containing the result of the Column expression.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql import SparkSession, functions as F\n        &gt;&gt;&gt; spark = SparkSession.builder.getOrCreate()\n\n        # Simple column expression\n        &gt;&gt;&gt; result = spark.column_function(F.lit(\"Hello, World!\"))\n        &gt;&gt;&gt; result.show()\n        +-------------+\n        |         col0|\n        +-------------+\n        |Hello, World!|\n        +-------------+\n\n        # Complex column expression\n        &gt;&gt;&gt; import datetime\n        &gt;&gt;&gt; complex_col = F.when(F.current_date() &gt; F.lit(datetime.date(2023, 1, 1)), \"Future\")\n        ...                .otherwise(\"Past\")\n        &gt;&gt;&gt; result = spark.column_function(complex_col)\n        &gt;&gt;&gt; result.show()\n        +------+\n        |  col0|\n        +------+\n        |Future|\n        +------+\n\n        # Using with user-defined functions (UDFs)\n        &gt;&gt;&gt; from pyspark.sql.types import IntegerType\n        &gt;&gt;&gt; square_udf = F.udf(lambda x: x * x, IntegerType())\n        &gt;&gt;&gt; result = spark.column_function(square_udf(F.lit(5)))\n        &gt;&gt;&gt; result.show()\n        +----+\n        |col0|\n        +----+\n        |  25|\n        +----+\n\n    Notes:\n        - This function is particularly useful for debugging or testing Column expressions\n          without the need to create a full DataFrame.\n        - The resulting DataFrame will always have a single column named 'col0' unless\n          the input Column object has a specific alias.\n        - Be cautious when using this with resource-intensive operations, as it still\n          creates a distributed DataFrame operation.\n    \"\"\"\n    return spark.range(1).select(column_obj)\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.convert_1d_list_to_dataframe","title":"<code>convert_1d_list_to_dataframe(spark, list_, column_names, axis='column')</code>","text":"<p>Converts a 1-dimensional list into a PySpark DataFrame.</p> <p>This function takes a 1-dimensional list and converts it into a PySpark DataFrame with the specified column names. The list can be converted into a DataFrame with either a single column or a single row, based on the specified axis.</p> <p>Parameters: spark (SparkSession): The Spark session to use for creating the DataFrame. list_ (list): The 1-dimensional list to convert. column_names (str or list of str): The name(s) of the column(s) for the DataFrame. axis (str): Specifies whether to convert the list into a single column or a single row.             Acceptable values are \"column\" (default) and \"row\".</p> <p>Returns: DataFrame: A PySpark DataFrame created from the 1-dimensional list.</p> <p>Raises: AttributeError: If the axis parameter is not \"column\" or \"row\".</p> <p>Example:</p> <p>spark = SparkSession.builder.appName(\"example\").getOrCreate() list_ = [1, 2, 3, 4] column_names = [\"numbers\"] df = convert_1d_list_to_dataframe(spark, list_, column_names, axis=\"column\") df.show() +-------+ |numbers| +-------+ |      1| |      2| |      3| |      4| +-------+ column_names = [\"ID1\", \"ID2\", \"ID3\", \"ID4\"] df = convert_1d_list_to_dataframe(spark, list_, column_names, axis=\"row\") df.show() +---+---+---+---+ |ID1|ID2|ID3|ID4| +---+---+---+---+ |  1|  2|  3|  4| +---+---+---+---+</p> Source code in <code>pysparky/spark_ext.py</code> <pre><code>@decorator.extension_enabler(SparkSession)\n@decorator.column_name_or_column_names_enabler(\"column_names\")\ndef convert_1d_list_to_dataframe(spark, list_, column_names, axis=\"column\"):\n    \"\"\"\n    Converts a 1-dimensional list into a PySpark DataFrame.\n\n    This function takes a 1-dimensional list and converts it into a PySpark DataFrame\n    with the specified column names. The list can be converted into a DataFrame with\n    either a single column or a single row, based on the specified axis.\n\n    Parameters:\n    spark (SparkSession): The Spark session to use for creating the DataFrame.\n    list_ (list): The 1-dimensional list to convert.\n    column_names (str or list of str): The name(s) of the column(s) for the DataFrame.\n    axis (str): Specifies whether to convert the list into a single column or a single row.\n                Acceptable values are \"column\" (default) and \"row\".\n\n    Returns:\n    DataFrame: A PySpark DataFrame created from the 1-dimensional list.\n\n    Raises:\n    AttributeError: If the axis parameter is not \"column\" or \"row\".\n\n    Example:\n    &gt;&gt;&gt; spark = SparkSession.builder.appName(\"example\").getOrCreate()\n    &gt;&gt;&gt; list_ = [1, 2, 3, 4]\n    &gt;&gt;&gt; column_names = [\"numbers\"]\n    &gt;&gt;&gt; df = convert_1d_list_to_dataframe(spark, list_, column_names, axis=\"column\")\n    &gt;&gt;&gt; df.show()\n    +-------+\n    |numbers|\n    +-------+\n    |      1|\n    |      2|\n    |      3|\n    |      4|\n    +-------+\n    &gt;&gt;&gt; column_names = [\"ID1\", \"ID2\", \"ID3\", \"ID4\"]\n    &gt;&gt;&gt; df = convert_1d_list_to_dataframe(spark, list_, column_names, axis=\"row\")\n    &gt;&gt;&gt; df.show()\n    +---+---+---+---+\n    |ID1|ID2|ID3|ID4|\n    +---+---+---+---+\n    |  1|  2|  3|  4|\n    +---+---+---+---+\n    \"\"\"\n    if axis not in [\"column\", \"row\"]:\n        raise AttributeError\n\n    if axis == \"column\":\n        tuple_list = ((x,) for x in list_)\n        output_sdf = spark.createDataFrame(tuple_list, schema=column_names)\n    elif axis == \"row\":\n        tuple_list = (tuple(list_),)\n        output_sdf = spark.createDataFrame(tuple_list, schema=column_names)\n    return output_sdf\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.convert_dict_to_dataframe","title":"<code>convert_dict_to_dataframe(spark, dict_, column_names, explode=False)</code>","text":"<p>Transforms a dictionary with list values into a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>dict</code> <p>The dictionary to transform. Keys will become the first column, and values will become the second column.</p> required <code>column_names</code> <code>list[str]</code> <p>A list containing the names of the columns.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A DataFrame with the dictionary keys and their corresponding exploded list values.</p> <p>Examples:</p> <p>datadict_ = {     \"key1\": 1,     \"key2\": 2 } column_names = [\"keys\", \"values\"] df = convert_dict_to_dataframe(datadict_, column_names) display(df)</p>"},{"location":"spark_ext/#pysparky.spark_ext.convert_dict_to_dataframe--key11","title":"key1,1","text":""},{"location":"spark_ext/#pysparky.spark_ext.convert_dict_to_dataframe--key22","title":"key2,2","text":"Source code in <code>pysparky/spark_ext.py</code> <pre><code>@decorator.extension_enabler(SparkSession)\ndef convert_dict_to_dataframe(\n    spark, dict_: dict[str, Any], column_names: list[str], explode: bool = False\n) -&gt; DataFrame:\n    \"\"\"\n    Transforms a dictionary with list values into a Spark DataFrame.\n\n    Args:\n        dict_ (dict): The dictionary to transform. Keys will become the first column, and values will become the second column.\n        column_names (list[str]): A list containing the names of the columns.\n\n    Returns:\n        pyspark.sql.DataFrame: A DataFrame with the dictionary keys and their corresponding exploded list values.\n\n    Examples:\n        datadict_ = {\n            \"key1\": 1,\n            \"key2\": 2\n        }\n        column_names = [\"keys\", \"values\"]\n        df = convert_dict_to_dataframe(datadict_, column_names)\n        display(df)\n        # key1,1\n        # key2,2\n    \"\"\"\n\n    # Assert that the input dictionary is not empty and column_names has exactly two elements\n    assert isinstance(dict_, dict), \"Input must be a dictionary\"\n    assert len(column_names) == 2, \"Column names list must contain exactly two elements\"\n\n    output_sdf = spark.createDataFrame(dict_.items(), column_names)\n\n    if explode:\n        assert all(\n            isinstance(val, list) for val in dict_.values()\n        ), \"All values in the dictionary must be lists\"\n        output_sdf = output_sdf.withColumn(column_names[1], F.explode(column_names[1]))\n\n    return output_sdf\n</code></pre>"},{"location":"spark_ext/#pysparky.spark_ext.createDataFrame_from_dict","title":"<code>createDataFrame_from_dict(spark, dict_)</code>","text":"<p>Creates a Spark DataFrame from a dictionary in a pandas-like style.</p> <p>Parameters:</p> Name Type Description Default <code>spark</code> <p>The SparkSession object.</p> required <code>dict_</code> <code>dict</code> <p>The dictionary to convert, where keys are column names and values are lists of column data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The resulting Spark DataFrame.</p> Source code in <code>pysparky/spark_ext.py</code> <pre><code>@decorator.extension_enabler(SparkSession)\ndef createDataFrame_from_dict(spark, dict_: dict) -&gt; DataFrame:\n    \"\"\"\n    Creates a Spark DataFrame from a dictionary in a pandas-like style.\n\n    Args:\n        spark: The SparkSession object.\n        dict_ (dict): The dictionary to convert, where keys are column names and values are lists of column data.\n\n    Returns:\n        DataFrame: The resulting Spark DataFrame.\n    \"\"\"\n    data = list(zip(*dict_.values()))\n    label = list(dict_.keys())\n    return spark.createDataFrame(data, label)\n</code></pre>"},{"location":"transformation_ext/","title":"Transformation ext","text":""},{"location":"transformation_ext/#pysparky.transformation_ext.apply_cols","title":"<code>apply_cols(sdf, col_func, cols=None, **kwargs)</code>","text":"<p>Apply a function to specified columns of a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>col_func</code> <code>callable</code> <p>The function to apply to each column.</p> required <code>cols</code> <code>list[str]</code> <p>List of column names to apply the function to.                         If None, applies to all columns. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A new Spark DataFrame with the function applied to the specified columns.</p> Source code in <code>pysparky/transformation_ext.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef apply_cols(\n    sdf: DataFrame, col_func: Callable, cols: list[str] = None, **kwargs\n) -&gt; DataFrame:\n    \"\"\"\n    Apply a function to specified columns of a Spark DataFrame.\n\n    Parameters:\n        sdf (DataFrame): The input Spark DataFrame.\n        col_func (callable): The function to apply to each column.\n        cols (list[str], optional): List of column names to apply the function to.\n                                    If None, applies to all columns. Defaults to None.\n\n    Returns:\n        DataFrame: A new Spark DataFrame with the function applied to the specified columns.\n    \"\"\"\n    if cols is None:\n        cols = sdf.columns\n    return sdf.withColumns(\n        {col_name: col_func(col_name, **kwargs) for col_name in cols}\n    )\n</code></pre>"},{"location":"transformation_ext/#pysparky.transformation_ext.distinct_value_counts_map","title":"<code>distinct_value_counts_map(sdf, column_name)</code>","text":"<p>Get distinct values from a DataFrame column as a map with their counts.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The name of the column to process.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame containing a single column with a map of distinct values and their counts.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [(\"Alice\",), (\"Bob\",), (\"Alice\",), (\"Eve\",), (None,)]\n&gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"name\"])\n&gt;&gt;&gt; result = distinct_value_counts_map(sdf, \"name\")\n&gt;&gt;&gt; result.show(truncate=False)\n+--------------------------+\n|name_map                  |\n+--------------------------+\n|{Alice -&gt; 2, Bob -&gt; 1, Eve -&gt; 1, NONE -&gt; 1}|\n+--------------------------+\n</code></pre> Source code in <code>pysparky/transformation_ext.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef distinct_value_counts_map(sdf: DataFrame, column_name: str) -&gt; DataFrame:\n    \"\"\"\n    Get distinct values from a DataFrame column as a map with their counts.\n\n    Args:\n        sdf (DataFrame): The input Spark DataFrame.\n        column_name (str): The name of the column to process.\n\n    Returns:\n        DataFrame: A DataFrame containing a single column with a map of distinct values and their counts.\n\n    Examples:\n        &gt;&gt;&gt; data = [(\"Alice\",), (\"Bob\",), (\"Alice\",), (\"Eve\",), (None,)]\n        &gt;&gt;&gt; sdf = spark.createDataFrame(data, [\"name\"])\n        &gt;&gt;&gt; result = distinct_value_counts_map(sdf, \"name\")\n        &gt;&gt;&gt; result.show(truncate=False)\n        +--------------------------+\n        |name_map                  |\n        +--------------------------+\n        |{Alice -&gt; 2, Bob -&gt; 1, Eve -&gt; 1, NONE -&gt; 1}|\n        +--------------------------+\n    \"\"\"\n    return (\n        sdf.select(column_name)\n        .na.fill(\"NONE\")\n        .groupBy(column_name)\n        .count()\n        .select(\n            F.map_from_entries(F.collect_list(F.struct(column_name, \"count\"))).alias(\n                f\"{column_name}_map\"\n            )\n        )\n    )\n</code></pre>"},{"location":"transformation_ext/#pysparky.transformation_ext.filters","title":"<code>filters(sdf, conditions, operator_=and_)</code>","text":"<p>Apply multiple filter conditions to a Spark DataFrame.</p> <p>This function takes a Spark DataFrame, a list of conditions, and an optional operator. It returns a new DataFrame with all conditions applied using the specified operator.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame to be filtered.</p> required <code>conditions</code> <code>list[Column]</code> <p>A list of Column expressions representing the filter conditions.</p> required <code>operator_</code> <code>Callable</code> <p>The operator to use for combining conditions. Defaults to <code>and_</code>. Valid options are <code>and_</code> and <code>or_</code>.</p> <code>and_</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pyspark.sql.DataFrame: A new DataFrame with all filter conditions applied.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported operator is provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from pyspark.sql.functions import col\n&gt;&gt;&gt; df = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['id', 'letter'])\n&gt;&gt;&gt; conditions = [col('id') &gt; 1, col('letter').isin(['b', 'c'])]\n</code></pre>"},{"location":"transformation_ext/#pysparky.transformation_ext.filters--filter-using-and-default-behavior","title":"Filter using AND (default behavior)","text":"<pre><code>&gt;&gt;&gt; filtered_df_and = filters(df, conditions)\n&gt;&gt;&gt; filtered_df_and.show()\n+---+------+\n| id|letter|\n+---+------+\n|  2|     b|\n|  3|     c|\n+---+------+\n</code></pre>"},{"location":"transformation_ext/#pysparky.transformation_ext.filters--filter-using-or","title":"Filter using OR","text":"<pre><code>&gt;&gt;&gt; filtered_df_or = filters(df, conditions, or_)\n&gt;&gt;&gt; filtered_df_or.show()\n+---+------+\n| id|letter|\n+---+------+\n|  2|     b|\n|  3|     c|\n|  1|     a|\n+---+------+\n</code></pre> Source code in <code>pysparky/transformation_ext.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef filters(\n    sdf: DataFrame, conditions: list[Column], operator_: Callable = and_\n) -&gt; DataFrame:\n    \"\"\"\n    Apply multiple filter conditions to a Spark DataFrame.\n\n    This function takes a Spark DataFrame, a list of conditions, and an optional\n    operator. It returns a new DataFrame with all conditions applied using the\n    specified operator.\n\n    Args:\n        sdf (pyspark.sql.DataFrame): The input Spark DataFrame to be filtered.\n        conditions (list[pyspark.sql.Column]): A list of Column expressions\n            representing the filter conditions.\n        operator_ (Callable, optional): The operator to use for combining\n            conditions. Defaults to `and_`. Valid options are `and_` and `or_`.\n\n    Returns:\n        pyspark.sql.DataFrame: A new DataFrame with all filter conditions applied.\n\n    Raises:\n        ValueError: If an unsupported operator is provided.\n\n    Examples:\n        &gt;&gt;&gt; from pyspark.sql.functions import col\n        &gt;&gt;&gt; df = spark.createDataFrame([(1, 'a'), (2, 'b'), (3, 'c')], ['id', 'letter'])\n        &gt;&gt;&gt; conditions = [col('id') &gt; 1, col('letter').isin(['b', 'c'])]\n\n        # Filter using AND (default behavior)\n        &gt;&gt;&gt; filtered_df_and = filters(df, conditions)\n        &gt;&gt;&gt; filtered_df_and.show()\n        +---+------+\n        | id|letter|\n        +---+------+\n        |  2|     b|\n        |  3|     c|\n        +---+------+\n\n        # Filter using OR\n        &gt;&gt;&gt; filtered_df_or = filters(df, conditions, or_)\n        &gt;&gt;&gt; filtered_df_or.show()\n        +---+------+\n        | id|letter|\n        +---+------+\n        |  2|     b|\n        |  3|     c|\n        |  1|     a|\n        +---+------+\n    \"\"\"\n    if operator_ not in (and_, or_):\n        raise ValueError(\n            f\"Unsupported operator: {operator_}. Valid options are 'and_' and 'or_.\"\n        )\n\n    default_value = F.lit(True) if operator_ == and_ else F.lit(False)\n    return sdf.filter(reduce(operator_, conditions, default_value))\n</code></pre>"},{"location":"transformation_ext/#pysparky.transformation_ext.get_latest_record_from_column","title":"<code>get_latest_record_from_column(sdf, window_partition_column_name, window_order_by_column_names, window_function=F.row_number)</code>","text":"<p>Fetches the most recent record from a DataFrame based on a specified column, allowing for custom sorting order.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The DataFrame to process.</p> required <code>window_partition_column_name</code> <code>str</code> <p>The column used to partition the DataFrame.</p> required <code>window_order_by_column_names</code> <code>str | list</code> <p>The column(s) used to sort the DataFrame.</p> required <code>window_function</code> <code>Column</code> <p>The window function for ranking records. Defaults to F.row_number.</p> <code>row_number</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with the most recent record for each partition.</p> Source code in <code>pysparky/transformation_ext.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef get_latest_record_from_column(\n    sdf: DataFrame,\n    window_partition_column_name: str,\n    window_order_by_column_names: str | list,\n    window_function: Column = F.row_number,\n) -&gt; DataFrame:\n    \"\"\"\n    Fetches the most recent record from a DataFrame based on a specified column, allowing for custom sorting order.\n\n    Parameters:\n        sdf (DataFrame): The DataFrame to process.\n        window_partition_column_name (str): The column used to partition the DataFrame.\n        window_order_by_column_names (str | list): The column(s) used to sort the DataFrame.\n        window_function (Column, optional): The window function for ranking records. Defaults to F.row_number.\n\n    Returns:\n        DataFrame: A DataFrame with the most recent record for each partition.\n    \"\"\"\n\n    if not isinstance(window_order_by_column_names, list):\n        window_order_by_column_names = [window_order_by_column_names]\n\n    return (\n        sdf.withColumn(\n            \"temp\",\n            window_function().over(\n                Window.partitionBy(window_partition_column_name).orderBy(\n                    *window_order_by_column_names\n                )\n            ),\n        )\n        .filter(F.col(\"temp\") == 1)\n        .drop(\"temp\")\n    )\n</code></pre>"},{"location":"transformation_ext/#pysparky.transformation_ext.get_unique_values","title":"<code>get_unique_values(df1, df2, column_name)</code>","text":"<p>Unions two DataFrames and returns a DataFrame with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>df1</code> <code>DataFrame</code> <p>First DataFrame.</p> required <code>df2</code> <code>DataFrame</code> <p>Second DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The column name containing the values.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>A DataFrame with unique values.</p> <p>Examples:</p>"},{"location":"transformation_ext/#pysparky.transformation_ext.get_unique_values--spark-sparksessionbuilderappnameuniquevaluesgetorcreate","title":"spark = SparkSession.builder.appName(\"UniqueValues\").getOrCreate()","text":""},{"location":"transformation_ext/#pysparky.transformation_ext.get_unique_values--df1-sparkcreatedataframe1-2-3-value","title":"df1 = spark.createDataFrame([(1,), (2,), (3,)], [\"value\"])","text":""},{"location":"transformation_ext/#pysparky.transformation_ext.get_unique_values--df2-sparkcreatedataframe3-4-5-value","title":"df2 = spark.createDataFrame([(3,), (4,), (5,)], [\"value\"])","text":""},{"location":"transformation_ext/#pysparky.transformation_ext.get_unique_values--unique_values-get_unique_valuesdf1-df2-value","title":"unique_values = get_unique_values(df1, df2, \"value\")","text":""},{"location":"transformation_ext/#pysparky.transformation_ext.get_unique_values--unique_valuesshow","title":"unique_values.show()","text":"Source code in <code>pysparky/transformation_ext.py</code> <pre><code>def get_unique_values(df1: DataFrame, df2: DataFrame, column_name: str) -&gt; DataFrame:\n    \"\"\"Unions two DataFrames and returns a DataFrame with unique values.\n\n    Args:\n        df1 (DataFrame): First DataFrame.\n        df2 (DataFrame): Second DataFrame.\n        column_name (str): The column name containing the values.\n\n    Returns:\n        DataFrame: A DataFrame with unique values.\n\n    Examples:\n        # spark = SparkSession.builder.appName(\"UniqueValues\").getOrCreate()\n        # df1 = spark.createDataFrame([(1,), (2,), (3,)], [\"value\"])\n        # df2 = spark.createDataFrame([(3,), (4,), (5,)], [\"value\"])\n        # unique_values = get_unique_values(df1, df2, \"value\")\n        # unique_values.show()\n    \"\"\"\n    # Union the DataFrames\n    union_df = df1.select(column_name).union(df2.select(column_name))\n\n    # Perform distinct to get unique values\n    unique_values_df = union_df.distinct()\n\n    return unique_values_df\n</code></pre>"},{"location":"transformation_ext/#pysparky.transformation_ext.transforms","title":"<code>transforms(sdf, transformations)</code>","text":"<p>Apply a series of transformations to a Spark DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>sdf</code> <code>DataFrame</code> <p>The input Spark DataFrame to be transformed.</p> required <code>transformations</code> <code>list</code> <p>A list of transformations, where each transformation is a tuple             containing a function and a dictionary of keyword arguments to apply the function to.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The transformed Spark DataFrame.</p> Source code in <code>pysparky/transformation_ext.py</code> <pre><code>@decorator.extension_enabler(DataFrame)\ndef transforms(\n    sdf: DataFrame, transformations: list[tuple[Callable, dict]]\n) -&gt; DataFrame:\n    \"\"\"\n    Apply a series of transformations to a Spark DataFrame.\n\n    Parameters:\n        sdf (DataFrame): The input Spark DataFrame to be transformed.\n        transformations (list): A list of transformations, where each transformation is a tuple\n                        containing a function and a dictionary of keyword arguments to apply the function to.\n\n    Returns:\n        DataFrame: The transformed Spark DataFrame.\n    \"\"\"\n    for transformation_funcs, kwarg in transformations:\n        assert callable(transformation_funcs), \"transformation_funcs must be callable\"\n        assert isinstance(kwarg, dict), \"kwarg must be a dictionary\"\n        sdf = sdf.transform(transformation_funcs, **kwarg)\n    return sdf\n</code></pre>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#pysparky.utils.create_map_from_dict","title":"<code>create_map_from_dict(dict_)</code>","text":"<p>Generates a PySpark map column from a provided dictionary.</p> <p>This function converts a dictionary into a PySpark map column, with each key-value pair represented as a literal in the map.</p> <p>Parameters:</p> Name Type Description Default <code>dict_</code> <code>Dict[str, int]</code> <p>A dictionary with string keys and integer values.</p> required <p>Returns:</p> Name Type Description <code>Column</code> <code>Column</code> <p>A PySpark Column object representing the created map.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dict_ = {\"a\": 1, \"b\": 2}\n&gt;&gt;&gt; map_column = create_map_from_dict(dict_)\n</code></pre> Source code in <code>pysparky/utils.py</code> <pre><code>def create_map_from_dict(dict_: dict[str, int]) -&gt; Column:\n    \"\"\"\n    Generates a PySpark map column from a provided dictionary.\n\n    This function converts a dictionary into a PySpark map column, with each key-value pair represented as a literal in the map.\n\n    Parameters:\n        dict_ (Dict[str, int]): A dictionary with string keys and integer values.\n\n    Returns:\n        Column: A PySpark Column object representing the created map.\n\n    Examples:\n        &gt;&gt;&gt; dict_ = {\"a\": 1, \"b\": 2}\n        &gt;&gt;&gt; map_column = create_map_from_dict(dict_)\n    \"\"\"\n\n    return F.create_map(list(map(F.lit, itertools.chain(*dict_.items()))))\n</code></pre>"},{"location":"utils/#pysparky.utils.join_dataframes_on_column","title":"<code>join_dataframes_on_column(column_name, *dataframes)</code>","text":"<p>Joins a list of DataFrames on a specified column using an outer join.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>The column name to join on.</p> required <code>*dataframes</code> <code>DataFrame</code> <p>A list of DataFrames to join.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The resulting DataFrame after performing the outer joins.</p> Source code in <code>pysparky/utils.py</code> <pre><code>def join_dataframes_on_column(\n    column_name: str, *dataframes: DataFrame | list[DataFrame]\n) -&gt; DataFrame:\n    \"\"\"\n    Joins a list of DataFrames on a specified column using an outer join.\n\n    Args:\n        column_name (str): The column name to join on.\n        *dataframes (DataFrame): A list of DataFrames to join.\n\n    Returns:\n        DataFrame: The resulting DataFrame after performing the outer joins.\n    \"\"\"\n\n    if not dataframes:\n        raise ValueError(\"At least one DataFrame must be provided\")\n\n    if isinstance(dataframes[0], list):\n        dataframes = dataframes[0]\n\n    # Check if all DataFrames have the specified column\n    if not all(column_name in df.columns for df in dataframes):\n        raise ValueError(f\"Column '{column_name}' not found in all DataFrames\")\n\n    # Use reduce to perform the outer join on all DataFrames\n    joined_df = reduce(\n        lambda df1, df2: df1.join(df2, on=column_name, how=\"outer\"), dataframes\n    )\n    return joined_df\n</code></pre>"},{"location":"utils/#pysparky.utils.union_dataframes","title":"<code>union_dataframes(*dataframes)</code>","text":"<p>Unions a list of DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>*dataframes</code> <code>DataFrame</code> <p>A list of DataFrames to union.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The resulting DataFrame after performing the unions.</p> Source code in <code>pysparky/utils.py</code> <pre><code>def union_dataframes(*dataframes: DataFrame | list[DataFrame]) -&gt; DataFrame:\n    \"\"\"\n    Unions a list of DataFrames.\n\n    Args:\n        *dataframes (DataFrame): A list of DataFrames to union.\n\n    Returns:\n        DataFrame: The resulting DataFrame after performing the unions.\n    \"\"\"\n    # TODO: Check on the schema, if not align, raise error\n\n    if not dataframes:\n        raise ValueError(\"At least one DataFrame must be provided\")\n\n    # Flatten the list if the first argument is a list of DataFrames\n    if isinstance(dataframes[0], list):\n        dataframes = dataframes[0]\n\n    return reduce(lambda df1, df2: df1.union(df2), dataframes)\n</code></pre>"}]}